{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-colab"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pathwaycom/pathway-examples/blob/main/showcases/live_vector_indexing_pipeline.ipynb\" target=\"_parent\"><img src=\"https://pathway.com/assets/colab-badge.svg\" alt=\"Run In Colab\" class=\"inline\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Pathway with Python 3.10+\n",
        "\n",
        "In the cell below, we install Pathway into a Python 3.10+ Linux runtime.\n",
        "\n",
        "> **If you are running in Google Colab, please run the colab notebook (Ctrl+F9)**, disregarding the 'not authored by Google' warning.\n",
        "> \n",
        "> **The installation and loading time is less than 1 minute**.\n"
      ],
      "metadata": {
        "id": "notebook-instructions"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-display\n",
        "!pip install --prefer-binary pathway"
      ],
      "metadata": {
        "id": "pip-installation-pathway",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "436ba6cf",
      "metadata": {},
      "source": [
        "# Always up-to-date data indexing pipeline\n",
        "\n",
        "This showcase shows how to use Pathway to deploy a live data indexing pipeline, which can be queried like a typical vector store. However, under the hood, Pathway updates the index on each data change, always giving up-to-date answers.\n",
        "<!-- canva link: https://www.canva.com/design/DAF1cxQW5Vg/LcFdDrPApBrgwM5kyirY6w/edit  -->\n",
        "::article-img\n",
        "---\n",
        "src: '/assets/content/showcases/vectorstore/vectorstore_doc.png'\n",
        "alt: 'Pathway data indexing pipeline'\n",
        "class: 'mx-auto'\n",
        "zoomable: true\n",
        "---\n",
        "::\n",
        "\n",
        "Pathway Vectorstore enables building a document index on top of you documents without the\n",
        "complexity of ETL pipelines, managing different containers for storing, embedding, and serving.\n",
        "It allows for easy to manage, always up-to-date, LLM pipelines accesible using a RESTful API\n",
        "and with integrations to popular LLM toolkits such as Langchain and LlamaIndex.\n",
        "\n",
        "\n",
        "In this article, we will use a simple document processing pipeline that:\n",
        "1. Monitors several data sources (files, S3 folders, cloud storages) for data changes.\n",
        "2. Parses, splits and embeds the documents.\n",
        "3. Builds a vector index for the data.\n",
        "\n",
        "However, If you prefer not to create the pipeline from the ground up and would like to check out the functionality,\n",
        "take a look at our [`managed pipelines`](https://cloud.pathway.com/docindex) in action.\n",
        "\n",
        "We will connect to the index using a `VectorStore` client, which allows retrieval of semantically similar documents."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ba393b4",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "Install the `pathway` package. You can also install the `unstructured` package to use the most powerful `unstructured.io`-based parser.\n",
        "\n",
        "Then download sample data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "379db4aa",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-21T07:39:49.037637Z",
          "iopub.status.busy": "2024-03-21T07:39:49.037456Z",
          "iopub.status.idle": "2024-03-21T07:39:49.646989Z",
          "shell.execute_reply": "2024-03-21T07:39:49.646671Z"
        }
      },
      "outputs": [],
      "source": [
        "!pip install pathway litellm\n",
        "# !pip install unstructured[all-docs]\n",
        "!mkdir -p sample_documents\n",
        "![ -f sample_documents/repo_readme.md ] || wget 'https://gist.githubusercontent.com/janchorowski/dd22a293f3d99d1b726eedc7d46d2fc0/raw/pathway_readme.md' -O 'sample_documents/repo_readme.md'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "03139804",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-21T07:39:49.648218Z",
          "iopub.status.busy": "2024-03-21T07:39:49.648119Z",
          "iopub.status.idle": "2024-03-21T07:39:49.650126Z",
          "shell.execute_reply": "2024-03-21T07:39:49.649860Z"
        }
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "import time\n",
        "\n",
        "logging.basicConfig(stream=sys.stderr, level=logging.WARN, force=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2705581c",
      "metadata": {},
      "source": [
        "## Building the data pipeline\n",
        "\n",
        "First, make sure you have an API key with an LLM provider such as OpenAI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "216c4461",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-21T07:39:49.651344Z",
          "iopub.status.busy": "2024-03-21T07:39:49.651157Z",
          "iopub.status.idle": "2024-03-21T07:39:49.652979Z",
          "shell.execute_reply": "2024-03-21T07:39:49.652711Z"
        }
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f9deb71",
      "metadata": {},
      "source": [
        "We will now assemble the data vectorization pipeline, using a simple `UTF8` file parser, a  character splitter and an embedder from the [Pathway LLM xpack](/developers/user-guide/llm-xpack/overview).\n",
        "\n",
        "First, we define the data sources. We use the files-based one for simplicity, but any supported `pathway` [connector](/developers/api-docs/pathway-io/), such as [s3](/developers/api-docs/pathway-io/s3/) or [Google Drive](/developers/api-docs/pathway-io/gdrive/#pathway.io.gdrive.read) will also work.\n",
        "\n",
        "Then, we define the embedder and splitter.\n",
        "\n",
        "Last, we assemble the data pipeline. We will start it running in a background thread to be able to query it immediately from the demonstration. Please note that in a production deployment, the server will run in another process, possibly on another machine. For the quick-start, we keep the server and client as different threads of the same Python process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "60e40003",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-21T07:39:49.654151Z",
          "iopub.status.busy": "2024-03-21T07:39:49.653965Z",
          "iopub.status.idle": "2024-03-21T07:39:54.825045Z",
          "shell.execute_reply": "2024-03-21T07:39:54.824672Z"
        }
      },
      "outputs": [],
      "source": [
        "import pathway as pw\n",
        "\n",
        "# This creates a connector that tracks files in a given directory.\n",
        "data_sources = []\n",
        "data_sources.append(\n",
        "    pw.io.fs.read(\n",
        "        \"./sample_documents\",\n",
        "        format=\"binary\",\n",
        "        mode=\"streaming\",\n",
        "        with_metadata=True,\n",
        "    )\n",
        ")\n",
        "\n",
        "# This creates a connector that tracks files in Google Drive.\n",
        "# Please follow the instructions at /developers/user-guide/connectors/gdrive-connector/ to get credentials.\n",
        "# data_sources.append(\n",
        "#     pw.io.gdrive.read(object_id=\"17H4YpBOAKQzEJ93xmC2z170l0bP2npMy\", service_user_credentials_file=\"credentials.json\", with_metadata=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "248e30d9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-21T07:39:54.826747Z",
          "iopub.status.busy": "2024-03-21T07:39:54.826416Z",
          "iopub.status.idle": "2024-03-21T07:39:55.641611Z",
          "shell.execute_reply": "2024-03-21T07:39:55.641260Z"
        }
      },
      "outputs": [],
      "source": [
        "# We now build the VectorStore pipeline\n",
        "\n",
        "from pathway.xpacks.llm.embedders import OpenAIEmbedder\n",
        "from pathway.xpacks.llm.splitters import TokenCountSplitter\n",
        "from pathway.xpacks.llm.vector_store import VectorStoreClient, VectorStoreServer\n",
        "\n",
        "PATHWAY_PORT = 8765\n",
        "\n",
        "# Choose document transformers\n",
        "text_splitter = TokenCountSplitter()\n",
        "embedder = OpenAIEmbedder(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "# The `PathwayVectorServer` is a wrapper over `pathway.xpacks.llm.vector_store` to accept LangChain transformers.\n",
        "# Fell free to fork it to develop bespoke document processing pipelines.\n",
        "vector_server = VectorStoreServer(\n",
        "    *data_sources,\n",
        "    embedder=embedder,\n",
        "    splitter=text_splitter,\n",
        ")\n",
        "vector_server.run_server(host=\"127.0.0.1\", port=PATHWAY_PORT, threaded=True, with_cache=False)\n",
        "time.sleep(30)  # Workaround for Colab - messages from threads are not visible unless a cell is running"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed31b5c1",
      "metadata": {},
      "source": [
        "We now instantiate and configure the client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5e31803d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-21T07:39:55.643300Z",
          "iopub.status.busy": "2024-03-21T07:39:55.643174Z",
          "iopub.status.idle": "2024-03-21T07:39:55.645140Z",
          "shell.execute_reply": "2024-03-21T07:39:55.644861Z"
        }
      },
      "outputs": [],
      "source": [
        "client = VectorStoreClient(\n",
        "    host=\"127.0.0.1\",\n",
        "    port=PATHWAY_PORT,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48a647a1",
      "metadata": {},
      "source": [
        "And we can start asking queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3735dba3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-21T07:39:55.646332Z",
          "iopub.status.busy": "2024-03-21T07:39:55.646123Z",
          "iopub.status.idle": "2024-03-21T07:39:55.647840Z",
          "shell.execute_reply": "2024-03-21T07:39:55.647561Z"
        },
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "query = \"What is Pathway?\"\n",
        "docs = client(query)\n",
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d19e3f22",
      "metadata": {},
      "source": [
        "**Your turn!** Now make a change to the source documents or make a fresh one and retry the query!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29101ba0",
      "metadata": {},
      "source": [
        "## Integrations\n",
        "\n",
        "### Langchain\n",
        "\n",
        "This currently is submitted to Langchain in [a PR](https://github.com/langchain-ai/langchain/pull/14859)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d7e5b933",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-21T07:39:55.649058Z",
          "iopub.status.busy": "2024-03-21T07:39:55.648860Z",
          "iopub.status.idle": "2024-03-21T07:39:55.650488Z",
          "shell.execute_reply": "2024-03-21T07:39:55.650219Z"
        }
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91dfea18",
      "metadata": {},
      "source": [
        "```python\n",
        "# PathwayVectorClient in Langchain is waiting for merging\n",
        "# in https://github.com/langchain-ai/langchain/pull/14859\n",
        "from langchain.vectorstores import PathwayVectorClient\n",
        "\n",
        "# PathwayVectorClient implements regular VectorStore API of LangChain\n",
        "client = PathwayVectorClient(host=\"127.0.0.1\", port=PATHWAY_PORT)\n",
        "docs = client.similarity_search(\"What is Pathway?\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "32fb6729",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-21T07:39:55.651863Z",
          "iopub.status.busy": "2024-03-21T07:39:55.651629Z",
          "iopub.status.idle": "2024-03-21T07:39:55.653394Z",
          "shell.execute_reply": "2024-03-21T07:39:55.653130Z"
        }
      },
      "outputs": [],
      "source": [
        "# Here we show how to configure a server that uses LangChain document processing components\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Choose proper LangChain document transformers\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "embeddings_model = OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "# Use VectorStoreServer.from_langchain_components to create a vector server using LangChain\n",
        "# document processors\n",
        "vector_server = VectorStoreServer.from_langchain_components(\n",
        "    *data_sources,\n",
        "    embedder=embeddings_model,\n",
        "    splitter=text_splitter,\n",
        ")\n",
        "vector_server.run_server(host=\"127.0.0.1\", port=PATHWAY_PORT+1, threaded=True, with_cache=False)\n",
        "time.sleep(30)  # colab workaround"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6bd744b6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-21T07:39:55.654506Z",
          "iopub.status.busy": "2024-03-21T07:39:55.654317Z",
          "iopub.status.idle": "2024-03-21T07:39:55.655975Z",
          "shell.execute_reply": "2024-03-21T07:39:55.655711Z"
        }
      },
      "outputs": [],
      "source": [
        "# You can connect to the Pathway+LlamaIndex server using any client - Pathway's, Langchain's or LlamaIndex's!\n",
        "client = VectorStoreClient(\n",
        "    host=\"127.0.0.1\",\n",
        "    port=PATHWAY_PORT+1,\n",
        ")\n",
        "\n",
        "client.query(\"pathway\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d991eff5",
      "metadata": {
        "lines_to_next_cell": 0
      },
      "source": [
        "### LlamaIndex\n",
        "\n",
        "Pathway is fully integrated with LlamaIndex! We show below how to instantiate a Llama-Index\n",
        "retriever that queries the Pathway VectorStoreServer\n",
        "and how to configure a server using LlamaIndex components.\n",
        "\n",
        "For more information see `Pathway Retriever`\n",
        "[cookbook](https://docs.llamaindex.ai/en/stable/examples/retrievers/pathway_retriever.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a915ecfb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-21T07:39:55.657068Z",
          "iopub.status.busy": "2024-03-21T07:39:55.656903Z",
          "iopub.status.idle": "2024-03-21T07:39:55.658564Z",
          "shell.execute_reply": "2024-03-21T07:39:55.658298Z"
        }
      },
      "outputs": [],
      "source": [
        "!pip install llama-index llama-index-retrievers-pathway llama-index-embeddings-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "76001660",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-21T07:39:55.659681Z",
          "iopub.status.busy": "2024-03-21T07:39:55.659464Z",
          "iopub.status.idle": "2024-03-21T07:39:55.661096Z",
          "shell.execute_reply": "2024-03-21T07:39:55.660840Z"
        }
      },
      "outputs": [],
      "source": [
        "# You can connect to the PathwayVectorStore using a llama-index compatible retriever\n",
        "from llama_index.retrievers.pathway import PathwayRetriever\n",
        "\n",
        "# PathwayRetriever implements the Retriever interface\n",
        "pr = PathwayRetriever(host=\"127.0.0.1\", port=PATHWAY_PORT)\n",
        "pr.retrieve(str_or_query_bundle=\"What is Pathway?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2dabc7be",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-21T07:39:55.662183Z",
          "iopub.status.busy": "2024-03-21T07:39:55.661993Z",
          "iopub.status.idle": "2024-03-21T07:39:55.663816Z",
          "shell.execute_reply": "2024-03-21T07:39:55.663546Z"
        }
      },
      "outputs": [],
      "source": [
        "# Here we show how to configure a server that uses LlamaIndex document processing components\n",
        "\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "\n",
        "# Choose proper LlamaIndex document transformers\n",
        "embed_model = OpenAIEmbedding(embed_batch_size=10)\n",
        "\n",
        "transformations_example = [\n",
        "    TokenTextSplitter(\n",
        "        chunk_size=150,\n",
        "        chunk_overlap=10,\n",
        "        separator=\" \",\n",
        "    ),\n",
        "    embed_model,\n",
        "]\n",
        "\n",
        "# Use VectorStoreServer.from_llamaindex_components to create a vector server using LlamaIndex\n",
        "# document processors\n",
        "vector_server = VectorStoreServer.from_llamaindex_components(\n",
        "    *data_sources,\n",
        "    transformations=transformations_example,\n",
        ")\n",
        "vector_server.run_server(host=\"127.0.0.1\", port=PATHWAY_PORT+2, threaded=True, with_cache=False)\n",
        "time.sleep(30)  # colab workaround"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ba25819e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-21T07:39:55.664914Z",
          "iopub.status.busy": "2024-03-21T07:39:55.664715Z",
          "iopub.status.idle": "2024-03-21T07:39:55.666355Z",
          "shell.execute_reply": "2024-03-21T07:39:55.666089Z"
        }
      },
      "outputs": [],
      "source": [
        "# You can connect to the Pathway+LlamaIndex server using any client - Pathway's, Langchain's or LlamaIndex's!\n",
        "client = VectorStoreClient(\n",
        "    host=\"127.0.0.1\",\n",
        "    port=PATHWAY_PORT+2,\n",
        ")\n",
        "\n",
        "client.query(\"pathway\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de866335",
      "metadata": {},
      "source": [
        "## Advanced topics\n",
        "\n",
        "### Getting information on indexed files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a95f604d",
      "metadata": {},
      "source": [
        "`PathwayVectorClient.get_vectorstore_statistics()` gives essential statistics on the state of the vector store, like the number of indexed files and the timestamp of the last updated one. You can use it in your chains to tell the user how fresh your knowledge base is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7b7a1984",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-21T07:39:55.667471Z",
          "iopub.status.busy": "2024-03-21T07:39:55.667286Z",
          "iopub.status.idle": "2024-03-21T07:39:55.668906Z",
          "shell.execute_reply": "2024-03-21T07:39:55.668640Z"
        }
      },
      "outputs": [],
      "source": [
        "client.get_vectorstore_statistics()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fab5490",
      "metadata": {},
      "source": [
        "### Filtering based on file metadata\n",
        "\n",
        "We support document filtering using [jmespath](https://jmespath.org/) expressions, for instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f32cf514",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-21T07:39:55.670046Z",
          "iopub.status.busy": "2024-03-21T07:39:55.669822Z",
          "iopub.status.idle": "2024-03-21T07:39:55.671514Z",
          "shell.execute_reply": "2024-03-21T07:39:55.671254Z"
        }
      },
      "outputs": [],
      "source": [
        "# take into account only sources modified later than unix timestamp\n",
        "docs = client(query, metadata_filter=\"modified_at >= `1702672093`\")\n",
        "\n",
        "# take into account only sources modified later than unix timestamp\n",
        "docs = client(query, metadata_filter=\"owner == `james`\")\n",
        "\n",
        "# take into account only sources with path containing 'repo_readme'\n",
        "docs = client(query, metadata_filter=\"contains(path, 'repo_readme')\")\n",
        "\n",
        "# and of two conditions\n",
        "docs = client(query, metadata_filter=\"owner == `james` && modified_at >= `1702672093`\")\n",
        "\n",
        "# or of two conditions\n",
        "docs = client(query, metadata_filter=\"owner == `james` || modified_at >= `1702672093`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7603a56",
      "metadata": {},
      "source": [
        "### Configuring the parser\n",
        "\n",
        "The vectorization pipeline supports pluggable parsers. If not provided, defaults to `UTF-8` parser. You can find available parsers [here](https://github.com/pathwaycom/pathway/blob/main/python/pathway/xpacks/llm/parser.py).\n",
        "An example parser that can read PDFs, Word documents and other formats is provided with `parsers.ParseUnstructured`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0fc3afe1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-21T07:39:55.672692Z",
          "iopub.status.busy": "2024-03-21T07:39:55.672444Z",
          "iopub.status.idle": "2024-03-21T07:39:55.674100Z",
          "shell.execute_reply": "2024-03-21T07:39:55.673838Z"
        }
      },
      "outputs": [],
      "source": [
        "# !pip install unstructured[all-docs]  # if you will need to parse complex documents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00b578e9",
      "metadata": {},
      "source": [
        "```python\n",
        "from pathway.xpacks.llm import parsers\n",
        "\n",
        "vector_server = VectorStoreServer(\n",
        "    *data_sources,\n",
        "    parser=parsers.ParseUnstructured(),\n",
        "    embedder=embeddings_model,\n",
        "    splitter=text_splitter,\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "183c5e9c",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "### Configuring the cache\n",
        "\n",
        "The Pathway vectorizing pipeline comes with an embeddings cache:\n",
        "```python\n",
        "vector_server.run_server(..., with_cache=True)\n",
        "```\n",
        "\n",
        "The default cache configuration is the locally hosted disk cache, stored in the `./Cache` directory. However, it can be customized by explicitly specifying the caching backend chosen among several persistent backend [options](/developers/api-docs/persistence-api/#pathway.persistence.Backend)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64a560ce",
      "metadata": {},
      "source": [
        "### Running in production\n",
        "\n",
        "A production deployment will typically run the server in a separate process. We provide a template application under [`templates`](https://github.com/langchain-ai/langchain/tree/master/templates/rag-pathway). We recommend running the Pathway data indexing pipeline in a container-based deployment environment like Docker or Kubernetes. For more info, see [Pathway's deployment guide](/developers/user-guide/deployment/docker-deployment/)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}